In all cases, written answers (apart from code) should not be longer than about three paragraphs.  Graders may not read all of your
submission if it is longer than that.

Homework reflection 1

1. In Coding Quiz 1, you are asked to find the distance of the farthest match in a set.  Is this farthest match distance too far to be a meaningful match?  How can you decide this?

h12["Z"].hist(bins=30, grid=False, edgecolor='black')
plt.xlabel("Z")
plt.ylabel("Frequency")
plt.suptitle("Histogram of Z, Mean: %.2f" % h12["Z"].mean())
plt.title("STD: %.2f" % h12["Z"].std())
plt.show()

treated = h12[h12["X"] == 1]
control = h12[h12["X"] == 0]
model = NearestNeighbors(n_neighbors = 1)
result = model.fit(control[["Z"]])
prediction = result.kneighbors(treated[["Z"]])
distances, indices = prediction

plt.hist(distances.flatten(), bins=30, edgecolor='black')
plt.xlabel("Distance to Nearest Neighbor")
plt.ylabel("Count")
plt.suptitle("Histogram of Distances to Nearest Neighbor (Z), Mean: %.2f" % distances.mean())
plt.title("STD: %.2f" % distances.std())
plt.show()

The max distance of approximately 0.21 is likely too far to be meaningful. I decided this by plotting a histogram of the Z values, calculating the standard deviation, and plotting a histrogram of the distances.
For the Z value distribution, it appears to have a mean around 0.5 with a standard deviation of approximately 0.3. This tells me that while the farthest distance is within one standard deviation of the Z values, it is closer to the higher end.
Next, I plotted a histogram of the distances. I discovered that the mean was approximately 0.05 and the standard deviation was 0.07. This is approximately 3 standard deviations away. Additionally, the 95th percentile is approximately 0.20.

2. In Coding Quiz 1, there are two approaches to matching: 
(A) Picking the best match X = 0 corresponding to each X = 1 using Z values.
(B) Using radius_neighbors to pick all matches X = 0 within a distance of 0.2 of each X = 1.

Invent your own type of matching similar to 1 and 2 (or look one up on the internet), which has a different way to pick the matches in X = 0.  Clearly explain the approach you invented or found.

# Split the data into treated (X=1) and control (X=0)
treated = h12[h12["X"] == 1].reset_index(drop=True)
control = h12[h12["X"] == 0].reset_index(drop=True)

control_Z = control["Z"].to_numpy()
control_Y = control["Y"].to_numpy()

# Store the effect for each treated unit
effects = []

# Loop through each treated row
for i in range(len(treated)):
    # Get this treated unit's Z and Y
    z_treated = treated.loc[i, "Z"]
    y_treated = treated.loc[i, "Y"]

    # Calculate distances in Z between this treated unit and all control units
    distances = np.abs(control_Z - z_treated)
    max_distance = distances.max()

    # Turn distances into weights
    # Exact match (distance = 0) weight = 1
    # Farthest control (distance = max_distance) weight = 0
    # Everyone else somewhere in between (linear scale)
    if max_distance == 0:
        weights = np.ones_like(distances, dtype=float)
    else:
        weights = 1.0 - (distances / max_distance)
        weights = np.clip(weights, 0.0, 1.0)

    # Weighted average outcome (Y) from the control group
    y_control_weighted = float((weights * control_Y).sum() / weights.sum())

    # Effect for this treated unit = treated Y minus weighted control Y
    effect = y_treated - y_control_weighted
    effects.append(effect)

# Average treatment effect across all treated units
average_effect = float(np.mean(effects))

print("Weighted matching effect (ATE):", round(average_effect, 6))

I created a method I call weighted matching. Instead of picking only one match or only the matches in a radius, I use all controls (X=0) and I gave them a weight based on how close they are to Z
in the treated unit. If Z is an exact match, the weight is 1. If Z is far away, the weight approaches 0. So every control is included, but the closer controls matter more in determining the effect.
This means that information is not lost, but that controls that are closer are prioritized. The weight is calculated by doing 1 - (distances / max_distance), with a clipping used to stay between 0 and 1.
Doing this on the quiz 1 homeowrk 1.2 data, I found an effect of approximately 0.69, which was higher than the effects found using nearest neighbor or the radius neighbors.

Homework reflection 2

1. Invent an example situation that would use fixed effects.

An example of when to use fixed effects is estimating the impact of the same gun control policy, like an assault weapons ban, across three countries over several years. We assume each country has traits that do not change over time and that affect the baseline level of gun deaths, like legal traditions, culture, geography, economy, and demographics.
With country fixed effects, we control for those baselines. This lets us isolate the policy’s effect. For example, we might estimate that the ban reduces gun deaths by 5 per 100,000 people per year on average, plus random noise with mean 0. Countries can start at different baseline levels,
but the policy effect is the same for all countries in this fixed effects model.

2. Write a Python program that performs a bootstrap simulation to find the variance in the mean of the Pareto distribution when different samples are taken.  Explain what you had to do for this.  As you make the full sample size bigger (for the same distribution), what happens to the variance of the mean of the samples?  Does it stay about the same, get smaller, or get bigger?

# Make results repeatable
np.random.seed(42)

# Pareto settings
pareto_shape = 2.5     # shape, must be > 2 for finite variance
pareto_minimum = 1.0   # minimum value

def draw_pareto_samples(num_samples, shape=pareto_shape, minimum=pareto_minimum):
    # Make num_samples from a Pareto(shape) with minimum value
    return (np.random.pareto(shape, size=num_samples) + 1.0) * minimum

def bootstrap_variance_of_mean(data, num_bootstrap_samples=2000):
    # Resample with replacement and record the mean each time
    sample_size = len(data)
    bootstrap_means = np.empty(num_bootstrap_samples)
    for i in range(num_bootstrap_samples):
        resampled_data = np.random.choice(data, size=sample_size, replace=True)
        bootstrap_means[i] = resampled_data.mean()
    return bootstrap_means.var(ddof=1), bootstrap_means  # sample variance

# Try different sample sizes
sample_sizes = [100, 200, 500, 1000, 5000, 10000]
variance_of_means = []

for sample_size in sample_sizes:
    data = draw_pareto_samples(sample_size)
    variance_estimate, _ = bootstrap_variance_of_mean(data, num_bootstrap_samples=2000)
    variance_of_means.append(variance_estimate)

# Plot: variance of the mean should go down as sample size grows
plt.plot(sample_sizes, variance_of_means, marker="o")
plt.xlabel("Sample size")
plt.ylabel("Variance of mean")
plt.title("Pareto(shape = 2.5): variance of the mean vs sample size")
plt.show()

# Table of results
results_df = pd.DataFrame({
    "sample_size": sample_sizes,
    "variance_of_mean": variance_of_means
})
print(results_df)

I set the shape to 2.5 and the minimum to 1 so the data is still heavy-tailed but has a real variance. For each sample size, I made one dataset. Then I did a bootstrap: I made many new “fake” samples by picking numbers from my dataset at random with replacement, which means I could pick the same number more than once. I found the average of each fake sample and looked at how
spread out those averages were. That spread is my estimate of the variance of the sample mean for that sample size. I set a random seed so someone else can repeat my steps and get the same results. When I compared different sample sizes, I saw that as the sample size got bigger, the variance of the mean got smaller. Bigger samples make the average steadier and
less fluctuating, which is what I expect when you average more numbers.

Homework reflection 3

1. In the event study in Coding Quiz 3, how would we go about testing for a change in the second derivative as well?

after = h31["After"].astype(int)              # 1 at/after event, 0 before
time = h31["time"].astype(float)              # original time
time_squared = time ** 2                      # time^2
time_after = after * time                     # slope change after
time_after_squared = after * time_squared     # curvature change after

curvature_rows = []
for series_name in series_list:
    y = h31[series_name].astype(float).to_numpy()
    X = pd.DataFrame({
        "time": time,
        "time_squared": time_squared,
        "after": after,
        "time_after": time_after,
        "time_after_squared": time_after_squared
    })
    X = sm.add_constant(X)
    model = sm.OLS(y, X).fit()
    curvature_rows.append({
        "series": series_name,
        "time_after_squared_coef": float(model.params["time_after_squared"]),
        "time_after_squared_se": float(model.bse["time_after_squared"]),
        "time_after_squared_t": float(model.tvalues["time_after_squared"]),
        "time_after_squared_p": float(model.pvalues["time_after_squared"]),
        "second_derivative_jump": float(2 * model.params["time_after_squared"])
    })

curvature_results = pd.DataFrame(curvature_rows)
curvature_results["abs_time_after_squared_coef"] = curvature_results["time_after_squared_coef"].abs()
curvature_sorted = curvature_results.sort_values(
    by=["time_after_squared_p", "abs_time_after_squared_coef"],
    ascending=[True, False]
).reset_index(drop=True)

print("Change in 2nd derivative at time = 50 (value ~ const + time + time^2 + after + time_after + time_after^2)")
print(curvature_sorted[["series", "time_after_squared_coef", "time_after_squared_se",
                        "time_after_squared_t", "time_after_squared_p",
                        "second_derivative_jump"]].to_string(index=False))
print(f"\nStrongest curvature change: {curvature_sorted.loc[0, 'series']}")

Testing a first derivative change asks “did the slope tilt after the event,” so you run a simple model with a constant, time, an “after” flag, and “time_after,” then you check the “time_after” coefficient. Testing a second derivative change asks “did the bend speed up or slow down after the event,” so you keep those pieces but also add “time_squared” and “time_after_squared,”
then you check the “time_after_squared” coefficient with a t test. If it is not zero, the bend changed, and the size of that bend change is two times that coefficient.

2. Create your own scenario that illustrates differences-in-differences. Describe the story behind the data and show whether there is a nonzero treatment effect.

# Basic settings
number_of_days = 1000
event_day = 500  # Dallas ends cashless bail here
slope_per_day = -0.01  # same downward trend in both cities
intercept_dallas = 20.0
intercept_fort_worth = 15.0
treatment_level_effect = -1.5  # extra drop in Dallas after the event

rows = []

# Build the panel for both cities
for city_name, city_intercept, is_treated in [
    ("Dallas", intercept_dallas, 1),
    ("Fort Worth", intercept_fort_worth, 0)
]:
    for day in range(number_of_days):
        is_post = 1 if day >= event_day else 0
        # Expected murders: parallel slopes, different intercepts, level drop after policy in Dallas
        murders = round(city_intercept + slope_per_day * day + is_treated * is_post * treatment_level_effect + np.random.normal(0, 2), 0)
        rows.append({
            "city": city_name,
            "day": day,
            "murders": murders,
            "treated": is_treated,  # Dallas=1, Fort Worth=0
            "post": is_post         # day >= 500 = 1
        })

# Final dataframe
did_data = pd.DataFrame(rows)

In the scenario, researchers tracked murders for 1,000 days in Dallas and Fort Worth. Both cities were already going down a little each day, and Dallas started higher by about five murders on average. On day 500, Dallas ended cashless bail. After that day, the regression says Dallas dropped extra compared with Fort Worth. The key number is the treated_post estimate of about −1.28,
which means Dallas had around 1.3 fewer murders per day than we would expect if nothing had changed. This estimate is precise enough that it is very unlikely to be zero, and a simple confidence interval runs from about −1.7 to −0.8. So yes, there is a real, nonzero treatment effect, and it is negative.

Homework reflection 4

1. The Coding Quiz gives two options for instrumental variables.  For the second item (dividing the range of W into multiple ranges), explain how you did it, show your code, and discuss any issues you encountered.

h41_bin = h41[(h41.W >= -1) & (h41.W <= -0.5) | (h41.W >= 0.5) & (h41.W <= 1)]
y_diff_bin = h41_bin[h41_bin.Z==1].Y.mean() - h41_bin[h41_bin.Z==0].Y.mean()
x_diff_bin = h41_bin[h41_bin.Z==1].X.mean() - h41_bin[h41_bin.Z==0].X.mean()
y_diff_bin / x_diff_bin

I created a binned dataframe by finding values of W (the confounder) that were between -1 and -0.5 or between 0.5 and 1. I then found the y difference by subtracting cases where the treatment was 0 from cases where the treatment was 1. I found the x difference by substracting the cases where the treatment was 0 from the cases where the treatment was 1.
I finally divided the y difference by the x difference to get the treatment effect of approximately 1.49. One issue is that I initially only included one range (>= -0.5 & <= 0.5), but this question says I should have multiple ranges. However, I recieved a very simialr result to using the full range of W ( approximately 1.56).


2. Plot the college outcome (Y) vs. the test score (X) in a small range of test scores around 80. On the plot, compare it with the Y probability predicted by logistic regression. The ground truth Y value is 0 or 1; don't just plot 0 or 1 - that will make it unreadable.  Find some way to make it look better than that.

h42a_bin77 = h42a[(h42a.X >= 77) & (h42a.X <= 77.99)]
h42a_bin78 = h42a[(h42a.X >= 78) & (h42a.X <= 78.99)]
h42a_bin79 = h42a[(h42a.X >= 79) & (h42a.X <= 79.99)]
h42a_bin80 = h42a[(h42a.X >= 80) & (h42a.X <= 80.99)]
h42a_bin81 = h42a[(h42a.X >= 81) & (h42a.X <= 81.99)]
h42a_bin82 = h42a[(h42a.X >= 82) & (h42a.X <= 82.99)]

h42a_bin78_y_mean = h42a_bin78.Y.mean()
h42a_bin79_y_mean = h42a_bin79.Y.mean()
h42a_bin80_y_mean = h42a_bin80.Y.mean()
h42a_bin81_y_mean = h42a_bin81.Y.mean()
h42a_bin82_y_mean = h42a_bin82.Y.mean()

model = smf.logit("Y ~ X", data=h42a).fit()
x_vals = numpy.linspace(77, 82, 100)
y_vals = model.predict(pd.DataFrame({"X": x_vals}))
plt.plot(x_vals, y_vals, label="Logistic Regression Prediction", color="blue")

bin_midpoints = [77.5, 78.5, 79.5, 80.5, 81.5, 82.5]
plt.plot(
    bin_midpoints,
    [
        h42a_bin77.Y.mean(),
        h42a_bin78_y_mean,
        h42a_bin79_y_mean,
        h42a_bin80_y_mean,
        h42a_bin81_y_mean,
        h42a_bin82_y_mean,
    ],
    marker="o",
    label="Bin Means",
)
plt.xticks(bin_midpoints, ["77-77.99", "78-78.99", "79-79.99", "80-80.99", "81-81.99", "82-82.99"])
plt.xlabel("X Bins")
plt.ylabel("Mean Y")
plt.title("Mean Y by Binned X Near Cutoff")
plt.axvline(x=80, color="r", linestyle="--", label="Cutoff at 80")
plt.legend()
plt.show()

I looked at test scores from 77 to 83. I put the scores into 1-point bins and plotted the average of Y in each bin, so I am not just showing a bunch of 0s and 1s. I also fit a logistic regression and drew its predicted chance as a smooth line. The red vertical line marks the cutoff at 80.
The points jump a lot right after 80 and then stay near about 0.6, while the blue line rises slowly, so I can compare the sharp change in the data with the logistic regression model.
