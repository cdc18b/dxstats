In all cases, written answers (apart from code) should not be longer than about three paragraphs.  Graders may not read all of your
submission if it is longer than that.

Homework Reflection 5

1. Draw a diagram for the following negative feedback loop:

Sweating causes body temperature to decrease.  High body temperature causes sweating.

A negative feedback loop means that one thing increases another while the second thing decreases the first.

Remember that we are using directed acyclic graphs where two things cannot directly cause each other.

2. Describe an example of a positive feedback loop.  This means that one things increases another while the second things also increases the first.

An example of a positive feedback loop is a viral post on social media. A viral post has a lot of shares. More shares cause more views. More views cause even more shares. More shares cause even more views. This is a positive feedback loop because the output feeds back into the input.

3. Draw a diagram for the following situation:

Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.
Bears eat deer, decreasing their population.
Deer eat flowers, decreasing their population.

Write a dataset that simulates this situation.  (Show the code.) Include noise / randomness in all cases.

Identify a backdoor path with one or more confounders for the relationship between deer and flowers.

One backdoor path is deer ← lightning_storm → flowers and the other is deer ← bears ← lightning_storm → flowers. The confounders are lightning_storm and flowers.

4. Draw a diagram for a situation of your own invention.  The diagram should include at least four nodes, one confounder, and one collider.  Be sure that it is acyclic (no loops).  Which node would say is most like a treatment (X)?  Which is most like an outcome (Y)?

In my invented directed acyclic graph, wildfires cause less get-out-the-vote (GOTV) operations, less voter turnout, and higher wait times at the polls. More GOTV operations cause higher voter turnout. Higher voter turnout causes longer wait times. Therefore, the treatment (X) is the GOTV operation, the outcome is voter turnout (Y), the confounder is the wildfire, and the collider is wait time.

Homework Reflection 6

1. What is a potential problem with computing the Marginal Treatment Effect simply by comparing each untreated item to its counterfactual and taking the maximum difference?  (Hint: think of statistics here.  Consider that only the most extreme item ends up being used to estimate the MTE.  That's not necessarily a bad thing; the MTE is supposed to come from the untreated item that will produce the maximum effect.  But there is nevertheless a problem.)
Possible answer: We are likely to find the item with the most extreme difference, which may be high simply due to randomness.
(Please explain / justify this answer, or give a different one if you can think of one.)

Picking the single largest matched difference is like running many tests and only keeping the biggest number, so chance alone can be deceptive. Even if true effect is low, random noise in outcomes and matching errors (nearest neighbor still somewhat far) can make one untreated item look huge by luck. This means the simple max will be biased and very unstable. If we took a new sample, a different item might be the highest and the estimate could jump a lot.
As the dataset gets bigger, the maximum tends to grow even when the real effect does not. Therefore, the taking the max treatment effect is likely too large and high variance unless you indicate that there is strong uncertainty.


2. Propose a solution that remedies this problem and write some code that implements your solution.  It's very important here that you clearly explain what your solution will do.
Possible answer: maybe we could take the 90th percentile of the treatment effect and use it as a proxy for the Marginal Treatment Effect.
(Either code this answer or choose a different one.)

Instead of taking the single biggest effect, use a high percentile of the untreated effects after matching, like the 90th percentile. This still targets a high effect, but it uses many data points, not just one. Therefore it reduces random noise, giving a more reliable estimate.


Homework Reflection 7

1. Create a linear regression model involving a confounder that is left out of the model.  Show whether the true correlation between $$X$$ and $$Y$$ is overestimated, underestimated, or neither.  Explain in words why this is the case for the given coefficients you have chosen.

W affects both X and Y, so it is a confounder. The true direct effect of X on Y is 1, but when you leave W out and regress Y only on X, the model wrongly assigns some of W’s effect to X. Because X tends to be high when W is high, and W also pushes Y up by 2, the estimated slope on X becomes about 2 on average.
So the X effect is overestimated. The correlation between X and Y is about 0.853 in this setup. Sample correlations will be around that number, not consistently above or below it. The bias shows up in the regression slope when the confounder is omitted.

2. Perform a linear regression analysis in which one of the coefficients is zero, e.g.

W = [noise]
X = [noise]
Y = 2 * X + [noise]

And compute the p-value of a coefficient - in this case, the coefficient of W.  
(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)
If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)
Run the analysis 1000 times and report the best (smallest) p-value.  
If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?

Here the true coefficient of W is zero. A p-value is the chance of getting an estimate at least this extreme if the true effect is zero. If you run one test, about 5% of p-values will be below 0.05 just by luck. If you run the whole analysis 1,000 times and only report the smallest p-value,
you will almost always find a very tiny number even though W truly has no effect. That tiny p-value does not prove W matters. The problem is repeating the analysis and cherry-picking the best result, which leads to false positives.

Homework Reflection 8

Include the code you used to solve the two coding quiz problems and write about the obstacles / challenges / insights you encountered while solving them.

Coding Quiz 5, Question 2:

I wanted to see what happens when I control for a collider, so I kept the same structure where “difficulty” and “speed” both helped determine “accident.” I ran it 10,000 times with 100,000 rows each time so that the average would settle, although I noticed right away that this was computationally expensive and time consuming.
I could have started with smaller numbers first. The main challenge was reminding myself that the bad part is not the regression code but the causal mechanism. By conditioning on “accident,” which is caused by both the predictor and the outcome, I opened a backdoor path and pulled the coefficient on difficulty away from the true data generating process.
I also checked that the average coefficient was about -10, which lined up with the way I subtracted difficulty times ten when I created speed, so the result was not random but created by my own collider mistake. This was a good reminder that “controls” are not always good and that causal diagrams should come before code.

Coding Quiz 6, Question 2:

For this one I was careful about which group to match to which, because I wanted the average treatment tffect on the treated (ATT) and not the effect on the untreated (ATU). That meant I had to fit the nearest neighbor model on the control group and then find the closest control unit for every treated unit. At first it was easy to flip that by accident, which would have given me the ATU.
The other small challenge was remembering to match on Z only, since that was the covariate available, and then to pull the matched Y values from the control rows using the returned indices. Once I did that, the ATT was about 1.8. The bigger insight was that the code looks like simple sklearn matching, but the interpretation is causal only because I was explicit about which group defines the target population. 